{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd Section: Optimization of the chosen algorithm and validation on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Scikit Learn libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscelaneous libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 (re) Load dataset and perform one transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quick function to load the data and make the few transformations\n",
    "def load_transform(filepath):\n",
    "    '''\n",
    "    Quick function to load the data and drop one column\n",
    "    INPUT:\n",
    "    filepath of data to be loaded\n",
    "    OUTPUT:\n",
    "    dataframe ready for ML Pipeline\n",
    "        '''\n",
    "    \n",
    "    # load dataframe\n",
    "    df = pd.read_csv(filepath, sep = \";\")\n",
    "    \n",
    "    # drop column coming from saving date\n",
    "    df.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2017 data\n",
    "df = load_transform(\"cleaned_data_2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 (re) Prepare X, y as well as training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare training and testing set\n",
    "def prep_train_test(df, test_size = 0.25):\n",
    "    '''\n",
    "    Extract X and y, Split data into training and testing set\n",
    "    INPUT:\n",
    "    df: dataframe\n",
    "    test_size (float): share of testing set \n",
    "    OUTPUT:\n",
    "    X_train, X_test, y_train, y_test\n",
    "    '''\n",
    "    \n",
    "    # Extract X and y\n",
    "    y = df['rad'].values\n",
    "    X = df.drop(['rad', 'siren'], axis = 1).values\n",
    "    \n",
    "    # Split training and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and testing set\n",
    "X_train, X_test, y_train, y_test = prep_train_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Model optimization using GridSearchCV\n",
    "\n",
    "In the previous section we saw that the RandomForestClassifier is having a relatively good performance (aka in our case precision). We will now try to further optimize by tuning the hyperparameters of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline model with a scaler and a RandomForestClassifier\n",
    "model = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New test to try on precision\n",
    "param_grid_new = {\"clf__n_estimators\" : [10,20], \n",
    "              \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "              \"clf__max_depth\": [None, 5],\n",
    "              \"clf__bootstrap\": [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV] clf__bootstrap=True, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__bootstrap=True, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=10, score=0.08004778972520908, total=  51.3s\n",
      "[CV] clf__bootstrap=True, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   54.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__bootstrap=True, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=10, score=0.07981220657276995, total=  45.8s\n",
      "[CV] clf__bootstrap=True, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__bootstrap=True, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=20, score=0.0886426592797784, total= 1.6min\n",
      "[CV] clf__bootstrap=True, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__bootstrap=True, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=20, score=0.09117647058823529, total= 1.4min\n",
      "[CV] clf__bootstrap=True, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  4.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__bootstrap=True, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=10, score=0.0, total=  10.0s\n",
      "[CV] clf__bootstrap=True, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=10, score=0.0, total=  10.3s\n",
      "[CV] clf__bootstrap=True, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=20, score=0.0, total=  13.8s\n",
      "[CV] clf__bootstrap=True, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=20, score=0.0, total=  14.5s\n",
      "[CV] clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=10, score=0.08870967741935484, total=  48.3s\n",
      "[CV] clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=10, score=0.08392603129445235, total=  47.8s\n",
      "[CV] clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=20, score=0.09863429438543247, total= 1.5min\n",
      "[CV] clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=20, score=0.1042296072507553, total= 1.5min\n",
      "[CV] clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=10, score=0.0, total=   9.7s\n",
      "[CV] clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=10, score=0.0, total=   9.5s\n",
      "[CV] clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=20, score=0.0, total=  14.2s\n",
      "[CV] clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=True, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=20, score=0.0, total=  14.3s\n",
      "[CV] clf__bootstrap=False, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=10, score=0.061234817813765184, total= 1.0min\n",
      "[CV] clf__bootstrap=False, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=10, score=0.05555555555555555, total= 1.0min\n",
      "[CV] clf__bootstrap=False, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=20, score=0.05560747663551402, total= 1.9min\n",
      "[CV] clf__bootstrap=False, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=gini, clf__max_depth=None, clf__n_estimators=20, score=0.055152394775036286, total= 1.9min\n",
      "[CV] clf__bootstrap=False, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=10, score=0.0, total=  10.8s\n",
      "[CV] clf__bootstrap=False, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=10, score=0.0, total=  11.2s\n",
      "[CV] clf__bootstrap=False, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=20, score=0.0, total=  17.4s\n",
      "[CV] clf__bootstrap=False, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=gini, clf__max_depth=5, clf__n_estimators=20, score=0.0, total=  16.5s\n",
      "[CV] clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=10, score=0.05902951475737869, total= 1.1min\n",
      "[CV] clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=10, score=0.05405405405405406, total= 1.0min\n",
      "[CV] clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=20, score=0.055288461538461536, total= 2.1min\n",
      "[CV] clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=None, clf__n_estimators=20, score=0.05743740795287187, total= 2.2min\n",
      "[CV] clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=10, score=0.0, total=  11.8s\n",
      "[CV] clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=10 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=10, score=0.0, total=  12.3s\n",
      "[CV] clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=20, score=0.0, total=  19.3s\n",
      "[CV] clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=20 \n",
      "[CV]  clf__bootstrap=False, clf__criterion=entropy, clf__max_depth=5, clf__n_estimators=20, score=0.0, total=  19.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed: 26.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "      ...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'clf__n_estimators': [10, 20], 'clf__criterion': ['gini', 'entropy'], 'clf__max_depth': [None, 5], 'clf__bootstrap': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='precision', verbose=5)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate GridSearchCV\n",
    "model_final_prec = GridSearchCV(model, param_grid_new, verbose = 5, cv=2, scoring = 'precision')\n",
    "model_final_prec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the best model are:  {'clf__bootstrap': True, 'clf__criterion': 'entropy', 'clf__max_depth': None, 'clf__n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "# Print out parameters of best model to avoid having t re-run the exercise\n",
    "print(\"Parameters of the best model are: \",model_final_prec.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance to analyse which feature is most relevant\n",
    "importances_new = pd.DataFrame(model_final_prec.best_estimator_.named_steps[\"clf\"].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names from original dataframe\n",
    "X_index = df.drop(['rad', 'siren'], axis = 1).columns.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>publish</th>\n",
       "      <td>0.016618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delay</th>\n",
       "      <td>0.141137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.225692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_dept_2</th>\n",
       "      <td>0.003522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_dept_3</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              weight\n",
       "publish     0.016618\n",
       "delay       0.141137\n",
       "age         0.225692\n",
       "num_dept_2  0.003522\n",
       "num_dept_3  0.000000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign new index to importances dataframe\n",
    "importances_new.index = X_index\n",
    "importances_new.columns = [\"weight\"]\n",
    "feature_importance_new = importances\n",
    "feature_importance_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse which features are most important (sum is equal to 1)\n",
    "feature_importance_new = feature_importance_new.sort_values(by = \"weight\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight    0.861987\n",
       "dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many make the most of the weight\n",
    "feature_importance_new[:10].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.225692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legal_form_simple_Societe par actions simplifiee a associe unique</th>\n",
       "      <td>0.151295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delay</th>\n",
       "      <td>0.141137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_ape_47</th>\n",
       "      <td>0.090943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_ape_64</th>\n",
       "      <td>0.063907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legal_form_simple_Societe par actions simplifiee</th>\n",
       "      <td>0.054712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_ape_43</th>\n",
       "      <td>0.043968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_ape_62</th>\n",
       "      <td>0.038121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_dept_49</th>\n",
       "      <td>0.027228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_dept_35</th>\n",
       "      <td>0.024984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      weight\n",
       "age                                                 0.225692\n",
       "legal_form_simple_Societe par actions simplifie...  0.151295\n",
       "delay                                               0.141137\n",
       "new_ape_47                                          0.090943\n",
       "new_ape_64                                          0.063907\n",
       "legal_form_simple_Societe par actions simplifiee    0.054712\n",
       "new_ape_43                                          0.043968\n",
       "new_ape_62                                          0.038121\n",
       "num_dept_49                                         0.027228\n",
       "num_dept_35                                         0.024984"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_new[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**\n",
    "\n",
    "As expected the age of the company play a major role (which we could guess from the exploration phase as most of the companies going out of business were quite young).\n",
    "\n",
    "The legal form (as a proxy for capital structure and governance) also plays a role.\n",
    "\n",
    "The so called delay feature is also relevant, e.g. how long does it take a company to publish its results (often if those are bad, it takes longer to publish as it needs more alignment with auditors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_preds = model_final_prec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on test set: 13.002 %\n",
      "Accuracy on test set: 97.439 %\n"
     ]
    }
   ],
   "source": [
    "# Get performance on test set\n",
    "print(\"Precision on test set: {} %\".format(np.round(precision_score(y_test, y_preds)*100,3)))\n",
    "print(\"Accuracy on test set: {} %\".format(np.round(accuracy_score(y_test, y_preds)*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Precision on test set: 2.349 %\n",
      "Basic Accuracy on test set: 97.651 %\n"
     ]
    }
   ],
   "source": [
    "# Compute performance lift-off vs basic assumption\n",
    "print(\"Basic Precision on test set: {} %\".format(np.round(y_test.mean()*100,3)))\n",
    "print(\"Basic Accuracy on test set: {} %\".format(np.round((1-y_test.mean())*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance lift-off in terms of precision: 453.5 %\n",
      "Performance lift-off in terms of accuracy: -0.2 %\n"
     ]
    }
   ],
   "source": [
    "# Lift off\n",
    "perf_lift_off_prec = precision_score(y_test, y_preds) / y_test.mean() - 1\n",
    "perf_lift_off_acc = accuracy_score(y_test, y_preds) / (1-y_test.mean()) -1\n",
    "\n",
    "print(\"Performance lift-off in terms of precision: {} %\".format(np.round(perf_lift_off_prec*100,1)))\n",
    "print(\"Performance lift-off in terms of accuracy: {} %\".format(np.round(perf_lift_off_acc*100,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Test model on other years\n",
    "\n",
    "This is also interesting to see if the model also works for other years, as we would like to use it to predict on a year for which we don't yet have results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on 2016 financial information (and 2018 so called radiations)\n",
    "df_2016 = load_transform(\"cleaned_data_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract X and y (across the entire set, the entire set is now a test set)\n",
    "X_2016 = df_2016.drop(['siren', 'rad'], axis = 1)\n",
    "y_2016 = df_2016['rad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(827720, 189)\n",
      "(827720,)\n"
     ]
    }
   ],
   "source": [
    "# check sizes\n",
    "print(X_2016.shape)\n",
    "print(y_2016.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using model\n",
    "y_preds_2016 = model_final_prec.predict(X_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on 2016 data is then: 3.168 %\n",
      "Accuracy on 2016 data is then: 97.079 %\n"
     ]
    }
   ],
   "source": [
    "# Test on entire set\n",
    "print(\"Precision on 2016 data is then: {} %\".format(np.round(precision_score(y_2016, y_preds_2016)*100,3)))\n",
    "print(\"Accuracy on 2016 data is then: {} %\".format(np.round(accuracy_score(y_2016, y_preds_2016)*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Precision on test set: 2.667 %\n",
      "Basic Accuracy on test set: 97.333 %\n"
     ]
    }
   ],
   "source": [
    "# Compute performance lift-off vs basic assumption\n",
    "print(\"Basic Precision on test set: {} %\".format(np.round(y_2016.mean()*100,3)))\n",
    "print(\"Basic Accuracy on test set: {} %\".format(np.round((1-y_2016.mean())*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance lift-off in terms of precision: 18.8 %\n",
      "Performance lift-off in terms of accuracy: -0.3 %\n"
     ]
    }
   ],
   "source": [
    "# Lift off\n",
    "perf_lift_off_prec_2016 = precision_score(y_2016, y_preds_2016) / y_2016.mean() - 1\n",
    "perf_lift_off_acc_2016 = accuracy_score(y_2016, y_preds_2016) / (1-y_2016.mean()) - 1\n",
    "\n",
    "print(\"Performance lift-off in terms of precision: {} %\".format(np.round(perf_lift_off_prec_2016*100,1)))\n",
    "print(\"Performance lift-off in terms of accuracy: {} %\".format(np.round(perf_lift_off_acc_2016*100,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Save model\n",
    "\n",
    "for potential reuse without having to re-run it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "filename = 'finalized_model_2017-19.sav'\n",
    "pickle.dump(model_final_prec, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Load from file\\nwith open(pkl_filename, 'rb') as file:\\n    pickle_model = pickle.load(file)\\n    \\n\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2\n",
    "pkl_filename = \"pickle_model_2017-19.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model_final_prec, file)\n",
    "    \n",
    "'''\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
